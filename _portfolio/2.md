---
title: "MambaRALM: Analyzing RALMs with Selective State Space and Transformer Based Architectures for Long Sequence Modeling"
excerpt: "Natural Language Processing (CS 4650 at GA Tech, Spring 2024) course project."
collection: portfolio
---
## Abstract

This study examines the efficacy of Retrieval Augmented Language Models (RALMs), a recent paradigm incorporating retrievers to enhance standalone language models during inference. While most RALMs rely on transformer architecture, which suffers from scalability issues limiting context windows, this project explores the potential of the Mamba architecture, known for its proficiency with long sequences and Long Range Dependencies (LRDs), in improving RALMs' performance. The study constructs a RALM based on the Mamba architecture and evaluates it alongside a transformer-based RALM on a subset of the TriviaQA dataset. Results show comparable performance for small to medium context chunks ($k \leq 7$), but the Mamba-based RALM demonstrates better resilience to larger context sizes ($k > 7$), indicating its potential for handling irrelevant information more effectively.

## Manuscript and Code

- [Manuscript](https://github.com/abarton51/MambaRALM/blob/main/MambaRALM.pdf)

- [Code Repository](https://github.com/abarton51/MambaRALM)

