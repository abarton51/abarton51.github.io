---
title: "JMM '24 - Things I Learned"
date: 2024-01-05
excerpt: "Notes on Interesting Talks at the Joint Mathematics Meeting of 2024"
permalink: /posts/2024/01/jmm/
collection: posts
tags:
  - mathematics
  - machine learning
  - conference
---

I attended the 2024 Joint Mathematics Meeting to present a poster on research I did as part of an REU at NC State University. You can check out the research in the navigation bar.

The time I spent there with my friends and peers from the previous summer was amazing. Mathematicians from all over the world gathered to present cutting-edge research to their peers and bright-eyed students like myself. While I was there, I took notes on some fascinating concepts I learned about.

## VC Dimension

The **Vapnik-Chervonenkis dimension** (VC dimension) is a model capacity measurement used in statistics and machine learning. It serves as a measure of a model's capacity and is frequently used to guide the model selection process in machine learning applications. To understand the VC dimension, we must first understand shattering.[^1]

### Shattering

**Shattering** is the ability of a model to classify a set of points perfectly. More specifically:

- A model can create a function that divides points into two distinct classes without overlapping
- It differs from simple classification by considering all possible combinations of labels upon those points
- The VC dimension of a model is defined as the size of the largest set of points that the model can shatter

Consider a binary classification setup where:
- We have data from domain $\mathcal{X}$
- We want to predict $0$ or $1$ for each data point
- For a sample $\mathcal{C}\subseteq \mathcal{X}$ containing $n$ data points
- There are $2^n$ possible labelings (finite)

A hypothesis class $\mathcal{H}$ _shatters_ $\mathcal{C}$ if it can represent all $2^n$ of these functions. In other words, given any possible labeling of $\mathcal{C}$, we can find a hypothesis within $\mathcal{H}$ that achieves zero error.[^2]

**Important Notes:**
- Computing VC dimension becomes more complex with sophisticated models (e.g., neural networks)
- The VC dimension can be infinite
- An infinite VC dimension implies $\mathcal{H}$ is not PAC learnable
- This makes intuitive sense as an infinite VC dimension means the hypothesis space is too expressive to be meaningful

## PAC Learning

**Probably approximately correct learning** (PAC learning) provides a framework for mathematical analysis of machine learning. In this framework:

- The learner receives samples
- Must select a generalization function (hypothesis) from a function class
- Goals: high probability of low generalization error
- Must learn given any arbitrary:
  - Approximation ratio
  - Probability of success
  - Distribution of samples

### Key Terminology[^3]

Let's understand PAC learning through two examples:
1. Character recognition: Using an array of $n$ bits encoding a binary-valued image
2. Interval classification: Finding an interval to classify points as positive (inside) or negative (outside)

**Basic Concepts:**
- Instance space $\mathcal{X}$:
  - Character recognition: $\mathcal{X} = \{0, 1\}^n$
  - Interval problem: $\mathcal{X}$ = bounded intervals in $\mathbb{R}$

- Concept ($c\subseteq \mathcal{X}$):
  - Example 1: Patterns encoding letter "P"
  - Example 2: Open intervals $\{(a, b)\| 0 \leq a \leq \pi/2, \pi \leq b \leq \sqrt{13}\}$

- Concept class $\mathcal{C}$: Collection of concepts over $\mathcal{X}$

### PAC Learnability

For a concept class to be PAC learnable:

1. Let $EX(c, D)$ draw example $x$ using distribution $D$ and return label $c(x)$ (1 if $x\in c$, 0 otherwise)

2. Given $0 < \epsilon, \delta < 1$, there exists:
   - Algorithm $A$
   - Polynomial $p$ in $1/\epsilon, 1/\delta$
   - Sample size of $p$ drawn from $EX(c, D)$

3. With probability ≥ $1-\delta$:
   - $A$ outputs hypothesis $h\in C$
   - Average error ≤ $\epsilon$ on $\mathcal{X}$ with distribution $D$

If these conditions hold for all concepts $c\in C$, distributions over $\mathcal{X}$, and $0< \epsilon, \delta < 1$, then $C$ is **PAC learnable** and $A$ is a **PAC learning algorithm** for $C$.

[^1]: [What is the VC dimension?](https://www.educative.io/answers/what-is-the-vc-dimension)
[^2]: [Understanding VC Dimension](https://andrewcharlesjones.github.io/journal/vc-dimension.html)
[^3]: [Probably approximately correct learning](https://en.wikipedia.org/wiki/Probably_approximately_correct_learning)

## Summary

I learned a lot more than just this, but this is what I had the time, energy, and ability to understand enough to write some notes on.

