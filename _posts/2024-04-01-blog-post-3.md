---
title: "Notes on S4"
excerpt: "Notes on the S4 paper by Gu et al."
permalink: /posts/2024/04/01/blog-post-3.md/
collection: portfolio
---

## Effciently Modeling Long Sequences with Structured State Spaces: S4

- Central problem in sequence modeling is efficiently handling data that contains long-range dependencies (LRDs).

### Background: State Spaces

#### 2.1 SSMs: A Continuous-time Latent State Model

- The SSM is defined by the simple equation (1). It maps a 1-D input signal $u(t)$ to an N-D latent state $x(t)$ before projecting to a 1-D output signal $y(t)$. 
$$ \begin{gather}  x'(t) = Ax(t) + Bu(t) \\  y(t) = Cx(t) + Du(t) \end{gather}  $$
- Related to latent state models such as Hidden Markov Models.
- Goal: Use the SSM as a black-box representation in a deep sequence model, where $A, B, C, D$ are parameters learned by gradient descent.

#### Addressing Long-Range Dependencies with HiPPO

- HiPPO theory of continuous-time memorization.
- HiPPO specifies a class of certani matrices $A\in \mathbb{R}^{N\times N}$ that when incorporated into (1), allows that state $x(t)$ to memorize the history of the input $u(t)$.
- The most important matrix in this class is defined by equation (2), which we call the HiPPO matrix.
$$  \begin{gather} A_{nk} = -\begin{cases}  (2n+1)^{1/2}(2k+1)^{1/2}  & n > k \\ n+1 & n = k \\ 0 & n < k  \end{cases}   \end{gather}    $$

#### Discrete-time SSM: The Recurrent Representation

- To be applied on a discrete input sequence $(u_0, u_1, \ldots,)$ instead of a continuous $u(t)$, (1) must be discretized by a **step size** $\Delta$ that represents the resolution of the input. Conceptually, the inputs $u_k$ can be views as sampling an implicit underlying continuous signal $u(t)$, where $u_k = u(k\Delta)$.
- To discretize the continuous-time SSM, we follow prior work in using the bilinear method, which concerts the state matrix $A$ into an approximation $\overline{A}$. The discrete SSM is then,
$$ \begin{gather}  x_k = \overline{A}x_{k-1} + \overline{B}u_k\\ y_k = \overline{C}x_k   \end{gather}   $$
Equation (3) is now a sequence-to-sequence map $u_k\to y_k$ instead of a function-to-function. Moreover, the state equation is now a recurrence in $x_k$, allowing the discrete SSM to be computed like an RNN. Concretely, $x_k\in \mathbb{R}^N$ can be viewed as a hidden state with transition matrix $\overline{A}$.

#### 2.4 Training SSMs: Convolutional Representation

- The recurrent SSM (3) is not practical for training on modern hardware due to its sequential nature. Instead, there is a well-known connection between linear time-invariant (LTI) SSMs such as (1) and continuous convolutions. Correspondingly, (3) can be actually written as a discrete convolution.

- Let the initial state be $x_{-1} = 0$. Then unrolling (3) yields

$$ \begin{gather}   x_0 = \overline{B}u_0   & x_1 = \overline{AB}u_0 + \overline{B}u_1     & x_2 = \overline{A}^2\overline{B}u_0 + \overline{AB}u_1 + \overline{B}u_2 & \ldots \\    y_0 = \overline{CB}u_0 & y_1 = \overline{CAB}u_0 + \overline{CB}u_1 & \overline{C}\overline{A}^2\overline{B}u_0 + \overline{CAB}u_1  + \overline{CB}u_2 & \ldots \end{gather}    $$

This can be vectorized into a convolution (4) with an explicit formula for the convolution kernel

$$ \begin{gather}  y_k = \overline{C}\overline{A}^k\overline{B}u_0 + \overline{C}\overline{A}^{k-1}\overline{B}u_1 \ldots + \overline{CAB}u_{k-1} + \overline{CB}u_k     \\     y = \overline{K} * u  \\ \overline{K}\in \mathbb{R}^L  := \mathcal{K}_L(\overline{A}, \overline{B}, \overline{C}) := (\overline{C}\overline{A}^i\overline{B})_{i\in[L]} = (\overline{CB}, \overline{CAB}, \ldots, \overline{C}\overline{A}^{L-1}\overline{B}) \end{gather}    $$

In other words, equation (4) is a single non-circular convolution and can be computed very efficiently with FFTs, provided that $\overline{K}$ is known (pre-computed beforehand).

- However, computing $\overline{K}$ is non-trivial and is the focus of the technical contributions of S4. $\overline{K}$ is referred to as the **SSM convolution kernel** or filter.

### Method: Structure State Spaces (S4)

- Technical focus on developing the S4 parameterization and showing how to efficiently compute all views of the SSM: the continuous representation, the recurrent representation, and the convolutional representation.
- 3.1 is based on conjugation and diagonalization, and discusses why the naïve application does not work. 3.2 gives an overview of key technical components of their approach and formally defines the S4 parameterization. 3.3 sketches the main results.

#### 3.1 Motivation: Diagonalization

- Bottleneck in computing the discrete-time SSM (3) is that is involves repeated matrix multiplication by $\overline{A}$.
- To overcome this, they use a structural result that allows a simplification of the SSMs.

**Lemma 3.1** *Conjugation is an equivalence relation on SSMs* $(A, B, C) \sim (V^{-1}AV, V^{-1}B, CV)$.
*Proof*. Write out the two SSMs with state denoted by $x$ and $\tilde x$ respectively.

After multiplying the $\tilde x$ side SSM by $V$, the two SSMs become identical with $x = V\tilde x$. Therefore, these compute the exact same operator $u\to y$, but with a change of basis by $V$ in the state $x$.

Lemma 3.1 motivates putting $A$ into a canonical form by conjugation, which is ideally more structured and allows faster computation.

Unfortunately, the naïve application of diagonalization does not work due to numerical issues. They derive the explicit diagonalization for the HiPPO matrix (2) and show it has entries exponentially large in the state size $N$, rendering diagonalization numerically infeasible.  

**Lemma 3.2** *The HiPPO matrix $A$ in equation (2) is diagonalized by the matrix $V_{ij} = \binom{i+2}{i-j}$. In particular, $V_{3i, i} = \binom{4i}{2i} \approx 2^{4i}$. Therefore, $V$ has entries of magnitude up to $2^{4N/3}$.* 

#### 3.2 The S4 Parameterization: Normal Plus Low-Rank


